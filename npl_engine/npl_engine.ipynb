{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69692799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Anand\n",
      "[nltk_data]     Mall\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Anand\n",
      "[nltk_data]     Mall\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb86ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class TicketVectorizer:\n",
    "    def __init__(self, filepath):\n",
    "        self.df = pd.read_csv(filepath)\n",
    "        self.df['cleaned'] = self.df['description'].apply(clean_text)\n",
    "        \n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(self.df['cleaned'])\n",
    "\n",
    "    def transform_query(self, query):\n",
    "        cleaned_query = clean_text(query)\n",
    "        return self.vectorizer.transform([cleaned_query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16aaa535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class TicketRecommender:\n",
    "    def __init__(self, vectorizer):\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "    def recommend(self, query, top_k=3):\n",
    "        query_vec = self.vectorizer.transform_query(query)\n",
    "        \n",
    "        similarities = cosine_similarity(\n",
    "            query_vec,\n",
    "            self.vectorizer.tfidf_matrix\n",
    "        ).flatten()\n",
    "\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                \"ticket_id\": int(self.vectorizer.df.iloc[idx][\"ticket_id\"]),\n",
    "                \"description\": self.vectorizer.df.iloc[idx][\"description\"],\n",
    "                \"resolution\": self.vectorizer.df.iloc[idx][\"resolution\"],\n",
    "                \"similarity_score\": float(similarities[idx])\n",
    "            })\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89f63be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ticket_id': 156, 'description': 'Laptop not connecting to office WiFi network', 'resolution': 'Update network drivers, restart router and verify network security policies', 'similarity_score': 0.5663007807253793}\n",
      "{'ticket_id': 128, 'description': 'Laptop not connecting to office WiFi network', 'resolution': 'Update network drivers, restart router and verify network security policies', 'similarity_score': 0.5663007807253793}\n",
      "{'ticket_id': 144, 'description': 'Laptop not connecting to office WiFi network', 'resolution': 'Update network drivers, restart router and verify network security policies', 'similarity_score': 0.5663007807253793}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TicketVectorizer(\"../enterprise_synthetic_tickets.csv\")\n",
    "recommender = TicketRecommender(vectorizer)\n",
    "\n",
    "query = \"Laptop running Windows 11 unable to connect to office WiFi network from home network\"\n",
    "\n",
    "recommendations = recommender.recommend(query)\n",
    "\n",
    "for r in recommendations:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b950ffc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ticket_id': 128, 'description': 'Laptop not connecting to office WiFi network', 'category': 'WiFi', 'priority': 'High', 'resolution': 'Update network drivers, restart router and verify network security policies', 'similarity_score': 0.5579}\n",
      "{'ticket_id': 27, 'description': 'Frequent VPN disconnections while working from home network', 'category': 'VPN', 'priority': 'High', 'resolution': 'Clear cached VPN credentials, reauthenticate and verify MFA configuration', 'similarity_score': 0.3575}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Anand\n",
      "[nltk_data]     Mall\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Anand\n",
      "[nltk_data]     Mall\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# TEXT CLEANING\n",
    "# ==============================\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(word)\n",
    "        for word in tokens\n",
    "        if word not in stop_words and len(word) > 2\n",
    "    ]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# VECTORIZER\n",
    "# ==============================\n",
    "class TicketVectorizer:\n",
    "    def __init__(self, filepath):\n",
    "\n",
    "        self.df = pd.read_csv(filepath)\n",
    "\n",
    "        # ðŸ”¥ Remove duplicate descriptions\n",
    "        self.df = self.df.drop_duplicates(subset=[\"description\"])\n",
    "\n",
    "        # Clean text\n",
    "        self.df[\"cleaned\"] = self.df[\"description\"].apply(clean_text)\n",
    "\n",
    "        # ðŸ”¥ Use n-grams (important upgrade)\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            ngram_range=(1, 2),   \n",
    "            max_df=0.95,\n",
    "            min_df=1\n",
    "        )\n",
    "\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(self.df[\"cleaned\"])\n",
    "\n",
    "    def transform_query(self, query):\n",
    "        cleaned_query = clean_text(query)\n",
    "        return self.vectorizer.transform([cleaned_query])\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# RECOMMENDER\n",
    "# ==============================\n",
    "def recommend(self, query, top_k=3, threshold=0.30):\n",
    "\n",
    "    query_vec = self.vectorizer.transform_query(query)\n",
    "\n",
    "    similarities = cosine_similarity(\n",
    "        query_vec,\n",
    "        self.vectorizer.tfidf_matrix\n",
    "    ).flatten()\n",
    "\n",
    "    # ðŸ”¥ Boost same-category matches\n",
    "    query_lower = query.lower()\n",
    "\n",
    "    for idx, row in self.vectorizer.df.iterrows():\n",
    "        category = row[\"category\"].lower()\n",
    "\n",
    "        if category in query_lower:\n",
    "            similarities[idx] *= 1.25   # boost score\n",
    "        else:\n",
    "            similarities[idx] *= 0.90   # slight penalty\n",
    "\n",
    "    sorted_indices = similarities.argsort()[::-1]\n",
    "\n",
    "    results = []\n",
    "    for idx in sorted_indices:\n",
    "        if similarities[idx] < threshold:\n",
    "            continue\n",
    "\n",
    "        results.append({\n",
    "            \"ticket_id\": int(self.vectorizer.df.iloc[idx][\"ticket_id\"]),\n",
    "            \"description\": self.vectorizer.df.iloc[idx][\"description\"],\n",
    "            \"category\": self.vectorizer.df.iloc[idx][\"category\"],\n",
    "            \"priority\": self.vectorizer.df.iloc[idx][\"priority\"],\n",
    "            \"resolution\": self.vectorizer.df.iloc[idx][\"resolution\"],\n",
    "            \"similarity_score\": round(float(similarities[idx]), 4)\n",
    "        })\n",
    "\n",
    "        if len(results) == top_k:\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# RUN TEST\n",
    "# ==============================\n",
    "vectorizer = TicketVectorizer(\"../enterprise_synthetic_tickets.csv\")\n",
    "recommender = TicketRecommender(vectorizer)\n",
    "\n",
    "query = \"Laptop running Windows 11 unable to connect to office WiFi network from home network\"\n",
    "\n",
    "recommendations = recommender.recommend(query, top_k=3, threshold=0.35)\n",
    "\n",
    "for r in recommendations:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5151d52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
